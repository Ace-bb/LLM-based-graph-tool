## ç¯å¢ƒå‡†å¤‡
Clone this repository:
```bash
git clone https://github.com/OpenGVLab/InternVL.git
```
Create a conda virtual environment and activate it:
```bash
conda create -n internvl python=3.9 -y
conda activate internvl
```
Install dependencies using requirements.txt:
```bash
pip install -r requirements.txt
By default, our requirements.txt file includes the following dependencies:

-r requirements/internvl_chat.txt

-r requirements/streamlit_demo.txt

-r requirements/classification.txt

-r requirements/segmentation.txt
```
The clip_benchmark.txt is not included in the default installation. If you require the clip_benchmark functionality, please install it manually by running the following command:
```bash
pip install -r requirements/clip_benchmark.txt
```

è¿˜éœ€è¦é¢å¤–å®‰è£…ï¼š
Install flash-attn==2.3.6:
```bash
pip install flash-attn==2.3.6 --no-build-isolation
```
Alternatively you can compile from source:
```bash
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout v2.3.6
python setup.py install
```

## æ¨¡å‹ä¸‹è½½
æœ¬é¡¹ç›®ä¸­ä¸»è¦ä¸‹è½½çš„æ˜¯`1~8B`çš„æ¨¡å‹
#### å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (InternVL 2.0)

<table>
  <tr>
    <th>Model Name</th>
    <th>Vision Part</th>
    <th>Language Part</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
    <th>Document</th>
  </tr>
  <tr>
    <td>InternVL2&#8209;1B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT&#8209;300M&#8209;448px</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2-0.5B-Instruct">Qwen2&#8209;0.5B&#8209;Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-1B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-1B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;2B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT&#8209;300M&#8209;448px</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2-chat-1_8b">internlm2&#8209;chat&#8209;1&#8209;8b</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-2B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-2B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;4B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT&#8209;300M&#8209;448px</a></td>
    <td><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">Phi&#8209;3&#8209;mini&#8209;128k&#8209;instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-4B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-4B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;8B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT&#8209;300M&#8209;448px</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-7b-chat">internlm2_5&#8209;7b&#8209;chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-8B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-8B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;26B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2-chat-20b">internlm2&#8209;chat&#8209;20b</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-26B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-26B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;40B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>
    <td><a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous&#8209;Hermes&#8209;2&#8209;Yi&#8209;34B</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-40B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-40B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
  <tr>
    <td>InternVL2-Llama3-76B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>
    <td><a href="https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-70B">Hermesâ€‘2â€‘Thetaâ€‘<br>Llamaâ€‘3â€‘70B</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-Llama3-76B">ğŸ¤– link</a></td>
    <td><a href="https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html">ğŸ“– doc</a></td>
  </tr>
</table>


## æ„é€ è®­ç»ƒæ•°æ®é›†
æ„å»ºSFTæ•°æ®é›†ï¼Œå°†å…¨éƒ¨æ•°æ®é›†å­˜æ”¾åˆ°ä»»æ„ç›®å½•ï¼Œç„¶åæ„å»ºä¸€ä¸ªå¦‚ä¸‹æ‰€ç¤ºæ ¼å¼çš„`JSON`æ–‡ä»¶ï¼Œå­˜æ”¾åˆ°`internvl_chat/shell/data/`ç›®å½•ä¸‹ï¼Œ
```json
{
  "your-custom-dataset-1": {
    "root": "path/to/the/image/",
    "annotation": "path/to/the/jsonl/annotation",
    "data_augment": false,
    "repeat_time": 1,
    "length": "number of your data"
  },
  ...
}
```
ä¾‹å¦‚ï¼š
```json
{
  "sharegpt4v_instruct_gpt4-vision_cap100k": {
    "root": "playground/data/",
    "annotation": "playground/opensource/sharegpt4v_instruct_gpt4-vision_cap100k.jsonl",
    "data_augment": false,
    "repeat_time": 1,
    "length": 102025
  }
}
```
å»ºè®®æ˜¯å°†å®˜æ–¹çš„å¾®è°ƒæ•°æ®é›†ä¹ŸåŠ ä¸Šï¼Œè¿™æ ·èƒ½å¤Ÿåœ¨ä¿è¯å¢åŠ èƒ½åŠ›çš„åŒæ—¶ï¼Œä¿ç•™åŸæœ¬çš„èƒ½åŠ›ã€‚ä¸è¿‡æ•°æ®é›†çš„é…æ¯”éœ€è¦çœ‹ä¸ªäººã€‚å®˜æ–¹çš„å¾®è°ƒæ•°æ®é›†ä¸ºï¼š[InternVL-Chat-V1-2-SFT-Data](https://internvl.readthedocs.io/en/latest/internvl1.2/reproduce.html#training-datasets-preparation)
å¯ä»¥ç›´æ¥é€šè¿‡`HuggingFace`ä¸‹è½½ï¼š
```bash
https://huggingface.co/datasets/OpenGVLab/InternVL-Chat-V1-2-SFT-Data
```

åœ¨æ¯ä¸ªæ•°æ®é›†æ–‡ä»¶ä¸­ï¼Œ[å•æ¡æ•°æ®çš„æ ¼å¼](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html)å¦‚ä¸‹ï¼š
#### å•å¼ å›¾ç‰‡ï¼š
```json
{
  "id": 0,
  "image": "path/to/image.jpg",
  "width": 111,
  "height": 222,
  "conversations": [
    {"from": "human", "value": "<image>\nuser input"},
    {"from": "gpt", "text": "assistant output"},
    {"from": "human", "value": "user input"},
    {"from": "gpt", "text": "assistant output"}
  ]
}
```
å…¶ä¸­ï¼Œ`conversations`ä¸­çš„`<image>`æ ‡ç­¾è¡¨ç¤ºå›¾ç‰‡åœ¨å¯¹è¯ä¸­æ’å…¥çš„ä½ç½®ï¼Œå¹¶ä¸”æ•´ä¸ªæ•°æ®é›†ä¸­`<image>`çš„æ ‡ç­¾æ•°é‡å¿…é¡»å’Œå›¾ç‰‡çš„æ•°é‡ä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨å•å¼ å›¾ç‰‡çš„æ•°æ®é›†ä¸­ï¼Œ`<image>`æ ‡ç­¾åªèƒ½å‡ºç°ä¸€æ¬¡ã€‚

#### Grounding / Detection Data
å¯¹äºè¾“å‡ºè¾¹ç•Œæ¡†å’Œç›®æ ‡æ£€æµ‹çš„æ•°æ®ï¼Œå›¾ç‰‡çš„å¤§å°éƒ½éœ€è¦æ­£åˆ™åŒ–åˆ°`[0,1000]`ä¹‹é—´ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢è¿™ä¸ªå‡½æ•°è¿›è¡Œæ­£åˆ™åŒ–ï¼š
```python
def normalize_coordinates(box, image_width, image_height):
    x1, y1, x2, y2 = box
    normalized_box = [
        round((x1 / image_width) * 1000),
        round((y1 / image_height) * 1000),
        round((x2 / image_width) * 1000),
        round((y2 / image_height) * 1000)
    ]
    return normalized_box
```
å°†æ¨¡å‹çš„è¾¹ç•Œæ¡†è¾“å‡ºå®šä¹‰ä¸ºå¦‚ä¸‹æ ¼å¼ï¼š`<ref>class name</ref><box>[[x1, y1, x2, y2], ...]</box>`ã€‚
ä¸‹é¢æ˜¯ä¸¤ä¸ªæ•°æ®ç¤ºä¾‹ï¼Œè¾“å‡ºæœ‰å¤šä¸ªbboxçš„å¯ä»¥çœ‹ç¬¬äºŒä¸ªç¤ºä¾‹ï¼š
```json
[{
    "id": 2324,
    "image": "COCO_train2014_000000581857.jpg",
    "width": 427,
    "height": 640,
    "conversations": [
        {
            "from": "human",
            "value": "<image>\nPlease provide the bounding box coordinate of the region this sentence describes: <ref>the lady with the blue shirt</ref>"
        },
        {
            "from": "gpt",
            "value": "<ref>the lady with the blue shirt</ref><box>[[243, 469, 558, 746]]</box>"
        },
        {
            "from": "human",
            "value": "Please provide the bounding box coordinate of the region this sentence describes: <ref>woman in gray shirt facing camera on right</ref>"
        },
        {
            "from": "gpt",
            "value": "<ref>woman in gray shirt facing camera on right</ref><box>[[507, 409, 698, 740]]</box>"
        },
        {
            "from": "human",
            "value": "Please provide the bounding box coordinate of the region this sentence describes: <ref>a woman in glasses shops in an open air fruit market</ref>"
        },
        {
            "from": "gpt",
            "value": "<ref>a woman in glasses shops in an open air fruit market</ref><box>[[196, 422, 395, 708]]</box>"
        }
    ]
},
{
    "id": 78281,
    "image": "images/x00001541/000106464.jpg",
    "width": 800,
    "height": 800,
    "conversations": [
        {
            "from": "human",
            "value": "<image>\nPlease detect and label all objects in the following image and mark their positions."
        },
        {
            "from": "gpt",
            "value": "Sure, I will detect and label all objects in the image and mark their positions.\n\n```\n<ref>Bulletproof Helmet</ref><box>[[1, 2, 269, 235]]</box>\n<ref>Bulletproof Clothing</ref><box>[[650, 619, 990, 993]]</box>\n<ref>Gun Model</ref><box>[[32, 231, 977, 662]]</box>\n<ref>screw</ref><box>[[754, 376, 851, 429]]</box>\n<ref>handcuff</ref><box>[[698, 228, 931, 386]]</box>\n<ref>95 Type Assault Rifle</ref><box>[[39, 229, 983, 667]]</box>\n<ref>shield</ref><box>[[30, 597, 273, 993]]</box>\n<ref>telescope</ref><box>[[666, 38, 890, 217]]</box>\n<ref>Wireless Walkie-Talkie</ref><box>[[295, 2, 370, 226], [374, 0, 447, 226]]</box>\n<ref>bomb</ref><box>[[473, 61, 552, 181], [569, 61, 648, 183]]</box>\n<ref>weapon</ref><box>[[302, 617, 342, 993]]</box>\n<ref>vessel</ref><box>[[355, 653, 644, 991]]</box>\n<ref>artifact</ref><box>[[915, 0, 981, 294]]</box>\n```\n"
        }
    ]
}]
```
è¾“å‡ºæœ‰å¤šä¸ªè¾¹ç•Œæ¡†æ—¶çš„æ–‡æœ¬æ ·å¼ä¸ºï¼š
![bbox output example](./img/bbox_output_example.png)

æ„å»ºæµç¨‹å›¾æ£€æµ‹æ•°æ®é›†çš„ä»£ç è§ï¼š[data/construct_internvl_flowchart_datasets.py](data/construct_internvl_flowchart_datasets.py)


## å¼€å§‹è®­ç»ƒ
å¯åŠ¨å¾®è°ƒçš„å‘½ä»¤ä¸ºï¼š
```bash
# Using 8 GPUs, fine-tune the full LLM, cost about 30G per GPU
GPUS=8 PER_DEVICE_BATCH_SIZE=1 sh shell/internvl2.0/2nd_finetune/internvl2_2b_internlm2_1_8b_dynamic_res_2nd_finetune_full.sh
# Using 2 GPUs, fine-tune the LoRA, cost about 27G per GPU
GPUS=1 PER_DEVICE_BATCH_SIZE=1 sh shell/internvl2.0/2nd_finetune/internvl2_2b_internlm2_1_8b_dynamic_res_2nd_finetune_lora.sh
# Using 8 GPUs, fine-tune the LoRA, cost about 27G per GPU
GPUS=8 PER_DEVICE_BATCH_SIZE=1 sh shell/internvl2.0/2nd_finetune/internvl2_2b_internlm2_1_8b_dynamic_res_2nd_finetune_lora.sh
```

## åˆå¹¶æ¨ç†


## æ•ˆæœè¯„ä¼°




 [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
 [WARNING]  sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
 [WARNING]  using untested triton version (3.0.0), only 1.0.0 is known to be compatible